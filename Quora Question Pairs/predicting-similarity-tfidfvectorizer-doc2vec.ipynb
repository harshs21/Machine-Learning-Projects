{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ef6f5b13-18f7-21c0-4bbc-40f3f884b4b5"},"source":"# Predicting Similar Questions"},{"cell_type":"markdown","metadata":{"_cell_guid":"ccd13167-9f2a-8d3b-99e6-f32447bb3286"},"source":"The objective of this analysis is to use different Natural Language Processing methods to predict if pairs of questions have the same meaning. The data is from Quora and hosted on Kaggle: https://www.kaggle.com/quora/question-pairs-dataset. The sections of this analysis include:\n- Transforming the text\n- Method 1: TfidfVectorizer \n- Method 2: Doc2Vec"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb195f9c-1578-e37e-4be4-b05d8e3b54ba"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom gensim import utils\nfrom gensim.models.doc2vec import LabeledSentence\nfrom gensim.models import Doc2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb8285d2-aeb1-8720-3b87-169b1be959a0"},"outputs":[],"source":"df = pd.read_csv(\"../input/questions.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db62bd83-cf15-15b1-25ad-b036c2f8882c"},"outputs":[],"source":"# Use the full dataset on your personal computer.\n# I'm using a fraction so that it doesn't time out when uploading.\ndf = df[0:40000]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"def4f929-5770-91e3-e02b-5baf882e8b31"},"outputs":[],"source":"df.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3182574f-aef6-4ff3-6f73-a0da9ed563ae"},"outputs":[],"source":"df.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35c44a8f-30b8-885f-3421-7ed91103c006"},"outputs":[],"source":"df.is_duplicate.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c16a6f4-0a3c-c8b4-253f-26d415999b2a"},"outputs":[],"source":"25109/len(df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c11a54cc-3445-257f-8929-8af9d2092445"},"source":"Although accuracy won't be as good of a performance metric as F1, it's still good to establish some sort of a baseline. In this case, 62.8% will be our baseline for accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7ca166a-72d8-df36-d72e-271e9cf657c2"},"outputs":[],"source":"# Take a look at some of the question pairs.\nprint(\"Not duplicate:\")\nprint(df.question1[0])\nprint(df.question2[0])\nprint()\nprint(\"Not duplicate:\")\nprint(df.question1[1])\nprint(df.question2[1])\nprint()\nprint(\"Is duplicate:\")\nprint(df.question1[5])\nprint(df.question2[5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8e4b33a-9e11-336e-d325-95fecef2287a"},"source":"This task looks like it will be a little difficult since the first pair of questions have very similar wordings but different meanings, and the third pair have less similar wordings but the same meaning."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f1d3ec5-c78a-6675-fda3-6ada4339ada9"},"outputs":[],"source":"def review_to_wordlist(review, remove_stopwords=True):\n    # Clean the text, with the option to remove stopwords.\n    \n    # Convert words to lower case and split them\n    words = review.lower().split()\n\n    # Optionally remove stop words (true by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    \n    review_text = \" \".join(words)\n\n    # Clean the text\n    review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", review_text)\n    review_text = re.sub(r\"\\'s\", \" 's \", review_text)\n    review_text = re.sub(r\"\\'ve\", \" 've \", review_text)\n    review_text = re.sub(r\"n\\'t\", \" 't \", review_text)\n    review_text = re.sub(r\"\\'re\", \" 're \", review_text)\n    review_text = re.sub(r\"\\'d\", \" 'd \", review_text)\n    review_text = re.sub(r\"\\'ll\", \" 'll \", review_text)\n    review_text = re.sub(r\",\", \" \", review_text)\n    review_text = re.sub(r\"\\.\", \" \", review_text)\n    review_text = re.sub(r\"!\", \" \", review_text)\n    review_text = re.sub(r\"\\(\", \" ( \", review_text)\n    review_text = re.sub(r\"\\)\", \" ) \", review_text)\n    review_text = re.sub(r\"\\?\", \" \", review_text)\n    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n    \n    words = review_text.split()\n    \n    # Shorten words to their stems\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in words]\n    \n    review_text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(review_text)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8ef3145-5c43-781c-4097-bdb3a19158ad"},"outputs":[],"source":"def process_questions(question_list, questions, question_list_name):\n# function to transform questions and display progress\n    for question in questions:\n        question_list.append(review_to_wordlist(question))\n        if len(question_list) % 10000 == 0:\n            progress = len(question_list)/len(df) * 100\n            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24775bc7-a4b2-facd-e401-499575816c0a"},"outputs":[],"source":"questions1 = []     \nprocess_questions(questions1, df.question1, \"questions1\")\nprint()\nquestions2 = []     \nprocess_questions(questions2, df.question2, \"questions2\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"452c2930-1da6-47e0-8962-28f87071eebe"},"outputs":[],"source":"# Take a look at some of the processed questions.\nfor i in range(5):\n    print(questions1[i])\n    print(questions2[i])\n    print()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d816fea4-fe50-509d-3d50-0f56cde1c812"},"outputs":[],"source":"# Stores the indices of unusable questions\ninvalid_questions = []\nfor i in range(len(questions1)):\n    # questions need to contain a vowel (which should be part of a full word) to be valid\n    if not re.search('[aeiouyAEIOUY]', questions1[i]) or not re.search('[aeiouyAEIOUY]', questions2[i]):\n    # Need to subtract 'len(invalid_questions)' to adjust for the changing index values as questions are removed.\n        invalid_questions.append(i-len(invalid_questions))\nprint(len(invalid_questions))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"773c4ac9-f657-61d3-b9e0-301fe047fffc"},"outputs":[],"source":"# list of invalid questions\ninvalid_questions"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1f618f0-8f3c-a150-159e-4dbd532b866a"},"source":"These questions look pretty unusable, so it should be okay to remove them. Plus, we are only removing less than 0.09% of all of the questions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa18ae1e-af6d-5cfd-4ca3-eafca2ca5a61"},"outputs":[],"source":"# Remove the invalid questions\nfor index in invalid_questions:\n    df = df[df.id != index]\n    questions1.pop(index)\n    questions2.pop(index)\n\n# These questions are also unusable, but were not detected initially.\n# They were found when the function 'cosine_sim' stopped due to an error.\nunexpected_invalid_questions = [36460]#,42273,65937,304867,306828,353918] \nfor index in unexpected_invalid_questions:\n    df = df[df.id != index]\n    questions1.pop(index)\n    questions2.pop(index)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6702338b-04e5-c512-1186-d62a19c74715"},"outputs":[],"source":"# Use TfidfVectorizer() to transform the questions into vectors,\n# then compute their cosine similarity.\nvectorizer = TfidfVectorizer()\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"756d9766-cdca-e491-88ab-4dfaf6a3f6b3"},"outputs":[],"source":"Tfidf_scores = []\nfor i in range(len(questions1)):\n    score = cosine_sim(questions1[i], questions2[i])\n    Tfidf_scores.append(score)\n    if i % 10000 == 0:\n        progress = i/len(questions1) * 100\n        print(\"Similarity Scores is {}% complete.\".format(round(progress,2)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8ca24ce-7bad-6ddc-552e-986c35ead4e3"},"outputs":[],"source":"# Plot the scores\nplt.figure(figsize=(12,4))\nplt.hist(Tfidf_scores, bins = 200)\nplt.xlim(0,1)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85fc4149-21c5-6627-d484-675dbe158632"},"outputs":[],"source":"# Function to report the quality of the model\ndef performance_report(value, score_list):\n    # the value (0-1) is the cosine similarity score to determine if a pair of questions\n    # have the same meaning or not.\n    scores = []\n    for score in score_list:\n        if score >= value:\n            scores.append(1)\n        else:\n            scores.append(0)\n\n    accuracy = accuracy_score(df.is_duplicate, scores) * 100\n    print(\"Accuracy score is {}%.\".format(round(accuracy),1))\n    print()\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(df.is_duplicate, scores))\n    print()\n    print(\"Classification Report:\")\n    print(classification_report(df.is_duplicate, scores))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8ebbe6f-96ee-6ff8-b9d2-f5583672c33d"},"outputs":[],"source":"performance_report(0.52, Tfidf_scores)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee1ed460-137c-5e3c-9ceb-c4806bd6ad57"},"source":"Using a threshold of 0.51 for the cosine similarity maximizes both the f1-score and accuracy. It's good to see that we are scoring better than the baseline value of 63.1% accuracy. I'm not too surprised that we didn't score much above the baseline accuracy, given the difficulty of this task."},{"cell_type":"markdown","metadata":{"_cell_guid":"6633798e-9945-38d0-7dbd-82adf097c997"},"source":"## Method 2: Doc2Vec"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ab44252-b52f-5d3b-3cd8-e1cf4d78bb12"},"outputs":[],"source":"# Reset index to match the index values of questions1 and questions2\ndf = df.reset_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21c0e29e-ccf0-86b9-33cf-4d20000b6e88"},"outputs":[],"source":"# Contains the processed questions for Doc2Vec\nquestions_labeled = []\n\nfor i in range(len(questions1)):\n    # Question strings need to be separated into words\n    # Each question needs a unique label\n    questions_labeled.append(LabeledSentence(questions1[i].split(), df[df.index == i].qid1))\n    questions_labeled.append(LabeledSentence(questions2[i].split(), df[df.index == i].qid2))\n    if i % 10000 == 0:\n        progress = i/len(questions1) * 100\n        print(\"{}% complete\".format(round(progress, 2)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55bf95d8-fb0c-0646-3f22-65f2b0715a59"},"outputs":[],"source":"# Split questions for computing similarity and determining the lengths of the questions.\nquestions1_split = []\nfor question in questions1:\n    questions1_split.append(question.split())\n    \nquestions2_split = []\nfor question in questions2:\n    questions2_split.append(question.split())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7eab291-dce2-a1b0-6934-38e3333eb0b6"},"outputs":[],"source":"# Determine the length of questions to select more optimal parameters.\nlengths = []\nfor i in range(len(questions1_split)):\n    lengths.append(len(questions1_split[i]))\n    lengths.append(len(questions2_split[i]))\nlengths = pd.DataFrame(lengths, columns=[\"count\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22a38b79-1a80-f5e6-4e0e-987e07aefd94"},"outputs":[],"source":"lengths['count'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42fd0045-7b6c-cb63-d5f3-fd6b8ac93805"},"outputs":[],"source":"# 99% of the questions include 18 or fewer words.\nnp.percentile(lengths['count'], 99)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a854f9a1-967c-d8a2-0943-3616c90cde07"},"outputs":[],"source":"# Build the model\nmodel = Doc2Vec(dm = 1, min_count=1, window=10, size=150, sample=1e-4, negative=10)\nmodel.build_vocab(questions_labeled)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59ed1462-7671-dac5-6cbb-aa4007b28061"},"outputs":[],"source":"# Train the model\n# 20 epochs performs a bit better, but timed out when uploading\nfor epoch in range(5):\n    model.train(questions_labeled)\n    print(\"Epoch #{} is complete.\".format(epoch+1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98a7e6d0-515f-aa3b-8e8b-0820ae010332"},"outputs":[],"source":"# Check a few terms to ensure the model was trained properly.\nmodel.most_similar('good')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58c1bf60-7a43-dd3d-12a6-52b3260a398a"},"outputs":[],"source":"model.most_similar('peopl')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"661535d4-dcbd-5a35-c952-ddd6576f2e36"},"outputs":[],"source":"model.most_similar('book')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a25c1e94-7b8b-ff9b-9d6d-141c3b416971"},"source":"These words have appropriate similar words, so I am pleased with the training."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f96f6331-39f3-1ec8-6db4-a948c5aaa514"},"outputs":[],"source":"doc2vec_scores = []\nfor i in range(len(questions1_split)):\n    # n_similarity computes the cosine similarity in Doc2Vec\n    score = model.n_similarity(questions1_split[i],questions2_split[i])\n    doc2vec_scores.append(score)\n    if i % 10000 == 0:\n        progress = i/len(questions1_split) * 100\n        print(\"{}% complete.\".format(round(progress,2)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0085f3c-9682-334b-afae-eee818dd898f"},"outputs":[],"source":"# Plot the scores\nplt.figure(figsize=(12,4))\nplt.hist(doc2vec_scores, bins = 200)\nplt.xlim(0,1)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"174deafd-6537-5da7-e576-b0ffc09d6406"},"source":"It's interesting to see how Doc2Vec computes the pairs of questions to be more similar than TfidfVectorizer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef053598-9d46-f2de-3699-37efc3c13e56"},"outputs":[],"source":"performance_report(0.92, doc2vec_scores)"},{"cell_type":"markdown","metadata":{"_cell_guid":"62ea90fd-d732-dd87-70f6-8e3bd54b1ad4"},"source":"Using 0.92 as our threshold, we are able to score slightly higher with the Doc2Vec method. Accuracy is 2 percentage points higher and the f1-score increased by 0.01. Much like with TfidfVectorizer, it would have been nice to score higher, but this is by no means an easy challenge. Nonetheless, I hope that you have learned something from reading this and enjoyed this project as much as I did."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3c872d9-df0d-00e4-8a8b-655c51b11717"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}